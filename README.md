

<div align=center> <img src="./assert/logo.png" width=50%></div>

# Welcome to PhysVLM

</div>

üìñ**PhysVLM**: Enabling Visual Language Models to Understand Robotic Physical Reachability 

---

This is the official repository for PhysVLM. The goal of PhysVLM is to enable Vision-Language Models (VLMs) to understand robot reachability, with the future aim of making the action decisions generated by the model more reliable.

## Release

- [ ] Paper release [`üìïArxiv`](...).
- [ ] Release the Data and the Model.
- [ ] Release the Benchmark Phys-bench.
- [x] **`2025.02.27`** üî•**PhysVLM has been accepted to CVPR 2025.**
- [x] üî•Release the PhysVLM code.

## What can PhysVLM do now?
PhysVLM demonstrates advanced performance across reachability understanding„ÄÅEmbodiedQA„ÄÅVQA.

![](assert/tasks.png)

## Get Started

### 1.Clone & Install

```shell
git clone git@github.com:unira-zwj/PhysVLM.git
cd PhysVLM
pip install -e .
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

---


### 2.Download the PhysVLM models to the checkpoints folder.

| Model                              | Links                                  |
|---------                           |---------------------------------------|
| PhysVLM-3B (Ready)                 | [`ü§óHuggingFace`](...)    |
---


### 3.Inference

```shell
python start_physvlm_server.py
```

then you can request the server with `(app, host="0.0.0.0", port=8001)`

---

## Acknowledgement

- [LLaVA](https://github.com/haotian-liu/LLaVA/tree/main) provides the base codes.
- [qwen](https://github.com/QwenLM/Qwen2.5) provides the basic llm model.

## Citation
If you find PhysVLM useful for your research and applications, please cite using this BibTeX:
```bibtex
@inproceedings{zwj2025PhysVLM,
  title={PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability},
  author={Zhou, Weijie and Tao, Manli and Zhao, Chaoyang and Guo, Haiyun and Dong, Honghui and Tang, Ming and Wang, Jinqiao},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025},
  year={2025},
  organization={IEEE}
}